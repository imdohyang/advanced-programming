import { Injectable, HttpException, HttpStatus } from '@nestjs/common';
import { HttpService } from '@nestjs/axios';
import { firstValueFrom } from 'rxjs';

@Injectable()
export class LlmClientService {
  constructor(private readonly httpService: HttpService) {}

  async generateSummary(prompt: string): Promise<string> {
    const url = 'http://10.125.208.217:9241/v1/completions';
    
    // ìµœëŒ€ 3íšŒ ì¬ì‹œë„
    for (let attempt = 0; attempt < 3; attempt++) {
      try {
        console.log(`ğŸ¤– LLM í˜¸ì¶œ ì‹œë„ ${attempt + 1}/3`);
        
        const response = await firstValueFrom(
          this.httpService.post(url, {
            prompt: prompt.trim(),
            model: 'meta-llama/Llama-3.3-70B-Instruct',
            max_tokens: 1024,
            temperature: 0.3,
          }, {
            headers: {
              'Content-Type': 'application/json',
              Authorization: `Bearer dummy-key`,
            },
            timeout: 300000, // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
          })
        );

        const raw = response.data?.choices?.[0]?.text || '';
        
        if (!raw.trim()) {
          throw new Error('ë¹ˆ ì‘ë‹µ');
        }

        console.log(`âœ… LLM ì‘ë‹µ ì„±ê³µ (${raw.length}ì)`);
        return raw.trim();

      } catch (error) {
        console.error(`âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨ (${attempt + 1}/3):`, error.message);
        
        if (attempt < 2) {
          await new Promise(resolve => setTimeout(resolve, 2000 * (attempt + 1)));
        }
      }
    }

    throw new HttpException('LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨', HttpStatus.BAD_GATEWAY);
  }
}