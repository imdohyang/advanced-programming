import { Injectable, HttpException, HttpStatus } from '@nestjs/common';
import { HttpService } from '@nestjs/axios';
import { firstValueFrom } from 'rxjs';

@Injectable()
export class LlmClientService {
  constructor(private readonly httpService: HttpService) {}

  async generateSummary(prompt: string): Promise<string> {
    const url = 'http://10.125.208.217:9241/v1/completions';  // âœ… êµìˆ˜ë‹˜ ì„œë²„ ì£¼ì†Œ

    try {
      const response = await firstValueFrom(
        this.httpService.post(
          url,
          {
            prompt,                            // âœ… prompt ë°©ì‹
            model: 'meta-llama/Llama-3.3-70B-Instruct',  // âœ… ì„œë²„ì—ì„œ í™•ì¸ëœ modelëª…
            max_tokens: 1024, // ê°€ëŠ¥í•˜ë©´ìµœëŒ€ì¹˜ë¡œ
            temperature: 0.7,
          },
          {
            headers: {
              'Content-Type': 'application/json',
              Authorization: `Bearer dummy-key`, // âœ… ì•„ë¬´ ë¬¸ìì—´ì´ë©´ OK
            },
          }
        )
      );

      const raw = response.data?.choices?.[0]?.text || '';
      console.log('ğŸ§ª Raw LLM response:', raw);
      return raw.trim();
    } catch (err) {
      console.error('âŒ LLM ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨:', err?.message || err);
      throw new HttpException('LLM ìš”ì•½/í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨', HttpStatus.BAD_GATEWAY);
    }
  }
}
